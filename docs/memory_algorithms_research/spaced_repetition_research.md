# 艾宾浩斯遗忘曲线与间隔重复算法(SRS):科学原理、模型比较与个性化实践

## 导言:问题背景、研究范围与叙事框架

学习与记忆的效率,取决于两个相互交织的核心问题:一是记忆如何随时间衰减,二是如何在恰当时机施以复习以对抗遗忘。自艾宾浩斯在19世纪末用无意义音节系统刻画“遗忘曲线”以来,关于记忆保持与遗忘规律的定量研究已历时百余年,并逐步形成了从经典函数模型到现代概率预测与强化学习优化的一整套方法论与工程体系[^1]。与此同时,以SuperMemo为代表的间隔重复(Spaced Repetition System, SRS)算法,将“复习间隔调度”从经验策略推进到可计算、可优化的工程实践,深刻影响了Anki、墨墨背单词等主流学习软件的技术路线[^2]。

本报告以“是什么—如何做—有何意义”为叙事主线,聚焦以下研究范围:第一,艾宾浩斯遗忘曲线的经典与现代模型及其在真实学习场景中的适用性;第二,主流SRS算法(SM-2、SM-17、FSRS、DRL-SRS)的原理、参数化与工程实现;第三,基于用户表现的间隔动态调整策略与评估指标;第四,短期记忆向长期记忆转化的神经机制与SRS调度的对接;第五,个性化复习时间与回忆概率预测的算法设计;第六,真实大规模数据上的性能比较与实践建议;第七,分布效应导致的“曲线形状与遗忘率背离”的建模解释与教育启示。

核心结论预览如下。其一,现代分布模型揭示:经验遗忘曲线的形状不仅由“遗忘速率”决定,更受“记忆强度分布相对于回忆阈值的位置与散布”所支配,因而可能出现近似线性乃至凹形的曲线;这对“以刚好及格为目标”的学习策略提出了明确的否定性警示[^3]。其二,FSRS(Free Spaced Repetition Scheduler)在2万集合、7.38亿次复习的大规模数据上,以回忆概率预测的均方根误差(RMSE)为指标,表现出优于SM-2与接近或略优于SM-17的精度,且已在Anki生态中原生支持[^4][^5][^6][^7][^8]。其三,深度强化学习(DRL-SRS)将调度问题表述为马尔可夫决策过程,结合Transformer式半衰期回归(THLR)与仿真环境,在记忆预测与调度优化上取得显著提升,提示“预测+优化”一体化的未来方向[^9]。其四,工程落地层面,评分映射、目标回忆概率(如R=0.9)、RMSE/ARP评估、跨等级信息损失控制,是实现稳健调度与持续改进的关键抓手[^5][^7][^9]。

需要说明的信息缺口包括:SM-17的完整公式与参数未公开,现有描述多为高层原理;Anki默认调度与FSRS在统一数据集上的可复现实验细节仍需补充;神经科学机制与SRS调度的定量耦合缺乏直接实证;跨语言与跨任务泛化证据不足;评分等级映射对算法性能的影响尚需系统评估。这些限制将在后文相应位置予以讨论与界定。

---

## 遗忘曲线的数学模型:从经典到现代

艾宾浩斯以节省法测得“保持量随时间衰减”的曲线,奠定了遗忘研究的基础。现代研究在保留“时间—保持”核心关系的同时,进一步将“记忆强度分布”“回忆阈值”等要素纳入模型,从而解释为何经验曲线常与直观“指数衰减”设想不一致。

### 经典函数模型:指数、幂与Sigmoid

经典模型通常从以下三类函数出发:

- 指数模型:保持量随时间呈指数衰减,常以R(t) = e^(-t/S)描述,其中S为记忆稳定性参数。该模型形式简洁,参数含义直观,易于工程实现与参数估计[^10]。
- 幂函数模型:保持量随时间呈幂律衰减,R(t) ≈ c·t^(-b)。在某些材料与时间范围内,幂律能较好拟合长尾衰减现象,反映“初期遗忘快、后期趋于平缓”的特征[^10]。
- Sigmoid模型:当假设记忆强度服从正态分布,项目随时间向左(强度降低)移动,超过阈值方可回忆,则总体可回忆比例由正态分布的累积分布函数(CDF)给出,呈S形。Sigmoid在群体层面能描述“早期下降较慢—中期加速—后期再次趋缓”的复杂轨迹[^3]。

为直观比较三类模型的假设与适用性,见表1。

表1 遗忘曲线函数模型对比(指数/幂/Sigmoid)

| 模型 | 典型函数 | 关键参数 | 适用场景 | 优点 | 局限 |
|---|---|---|---|---|---|
| 指数 | R(t)=e^(-t/S) | S(稳定性) | 材料同质、时间较短、个体差异小 | 简洁、可解释、易实现 | 对长尾与群体异质性拟合欠佳 |
| 幂 | R(t)=c·t^(-b) | c(尺度)、b(指数) | 长时程、复杂材料 | 能拟合“初期快—后期慢” | 参数稳定性与跨任务迁移性受限 |
| Sigmoid | R(t)=Φ((μ-ct)/σ) | μ(阈值)、c(衰减)、σ(散布) | 群体层面、异质样本 | 体现“分布+阈值”机制 | 参数估计复杂、对数据质量要求高 |

注:Φ为标准正态CDF。指数与幂模型的参数化参考现代综述与实践总结;Sigmoid的分布-阈值解释见后文分布模型[^3][^10]。

### 分布模型与“曲线—速率”背离

最新分布模型指出,经验遗忘曲线与“个体遗忘率”之间存在系统背离:即使每个项目的遗忘率恒定,若记忆强度分布的均值接近回忆阈值且标准差较小,则群体层面的可回忆比例会随时间呈现近似线性、甚至凹形的曲线;反之,分布远离阈值或标准差较大时,曲线更接近传统的下凸形状[^3]。该模型还区分了“检索强度”(可提取性)与“存储强度”(持久性)两个维度,提示“刚好及格”的学习策略会因接近阈值而导致快速遗忘,强调应追求更高的安全边际与稳定增益[^3]。这一解释统一了“为何有时看到线性遗忘”的争议,并对教学与学习策略具有直接启示:不要将目标定位于“刚好通过”,而应通过间隔重复逐步抬升分布均值、降低散布,从而实现更稳健的长期保持。

### 曲线“变平”的机制与教育含义

间隔重复不仅延缓遗忘,更会在反复提取与巩固中“抬升”记忆强度分布,降低个体间差异,从而使后续遗忘曲线“变平”,即在更长的时间尺度上保持较高的可回忆比例[^11][^12]。教育含义在于:通过阶段性拉长间隔与穿插检索练习,逐步将“临界知识”转化为“稳健知识”,以减少考试或应用情境下的意外失误风险。

---

## SRS算法谱系与实现:SM-2、SM-17、FSRS、DRL-SRS

SRS的核心任务,是在“何时复习哪些项目”上做出最优或近优决策。不同算法在“如何建模记忆—如何更新参数—如何选择间隔—如何评估”上采用了各异的策略。

### SM-2:基于记忆质量的间隔调整

SM-2是第一代广泛使用的SRS算法,其核心思想是用一个“记忆难度因子”(E-Factor, EF)驱动间隔增长,并用0–5分的回忆质量评分(q)更新EF与间隔(I)[^13]。典型规则如下:

- 初始值:EF=2.5;首次间隔I(1)=1天;第二次I(2)=6天。
- 后续间隔:对于n>2,I(n)=I(n−1)×EF(四舍五入到整数天)。
- EF更新:EF' = EF + (0.1 − (5−q)×(0.08 + (5−q)×0.02)),并限制EF'≥1.3。
- 失败处理:若q<3,则该项目视为需要重学,并按规则缩短间隔重新安排。

SM-2将“评分—参数更新—间隔计算”形成闭环,简单且可解释,适合作为工程基线。表2给出质量评分与处理动作的常见映射。

表2 SM-2质量评分(0–5)与处理动作

| 评分q | 含义(示例) | 处理动作 |
|---|---|---|
| 5 | 完全轻松回忆 | 间隔按EF增长 |
| 4 | 略有迟疑但正确 | 间隔按EF增长 |
| 3 | 回忆困难但正确 | 轻微增长或维持,视EF而定 |
| 2 | 回忆错误但接近 | 缩短间隔,重学 |
| 1 | 回忆错误且较远 | 缩短间隔,重学 |
| 0 | 完全回忆不起来 | 最短间隔,重学 |

SM-2的优势在于实现简单、参数少、可解释性强;局限在于对复杂异质数据与多等级评分信息的利用不够充分,且对个体差异的刻画较粗[^13]。

### SM-17:理论到应用的转换

SM-17是SuperMemo系列的现代版本,强调将“记忆理论”转化为“实用调度算法”。受算法保密与数据不可得等限制,公开资料以高层描述为主:其目标是统一地刻画记忆的稳定性与可提取性,并据此优化间隔;但完整公式与参数未公开,外部难以在统一数据集上复现实证比较[^14][^15]。因此,关于SM-17的结论往往来自SuperMemo生态内部数据与经验总结,外部验证仍待未来开放数据与标准化评测推动。

### FSRS:概率预测驱动的调度

FSRS以“预测在特定时间点的回忆概率(R)”为核心,通过RMSE评估预测误差,并据此调度复习。在Anki生态的大规模数据(2万集合、7.38亿次复习,不含媒体与字段内容)上,FSRS v4默认参数已优于SM-2,并在与其他算法的对比中取得更低的RMSE,显示出较强的预测与调度精度[^4][^5][^6][^7][^8]。工程上,FSRS支持最多4个评分等级,这一设计在跨生态迁移时可能与6等级系统产生信息损失,需要在映射与优化中加以补偿[^5]。表3概述了公开报道中的数据集与评估要点。

表3 FSRS数据集与评估指标概览

| 指标/要素 | 内容 |
|---|---|
| 数据规模 | 2万Anki集合,7.38亿次复习(不含当日复习) |
| 数据字段 | 仅卡片ID、评分与间隔长度;不含媒体与卡片内容 |
| 评分等级 | 支持最多4等级(跨生态迁移时需处理信息损失) |
| 评估指标 | 以RMSE为主(预测R与实际R的平均差距) |
| 生态支持 | Anki桌面版、Web与移动端原生支持(Anki 23.10+) |

FSRS的实践意义在于:以概率预测为“度量—优化”的桥梁,使得调度器可围绕“目标回忆概率(如R=0.9)”进行显式控制与评估,从而实现数据驱动的持续改进[^5][^7]。

### DRL-SRS:强化学习优化调度

DRL-SRS将SRS调度问题表述为马尔可夫决策过程(MDP):状态包含项目难度、时间延迟与记忆强度等;动作为离散化的间隔选择;奖励基于当前回忆概率的提升。系统采用Transformer式的半衰期回归(THLR)进行记忆预测,结合DQN与LSTM在仿真环境中学习最优策略,在多个环境下取得更高的平均回忆概率(ARP)[^9]。关键结果见表4。

表4 DRL-SRS记忆预测与调度性能对比(节选)

| 模块/环境 | 指标 | DRL-SRS | 对比方法(节选) |
|---|---|---|---|
| THLR预测 | MAE | 0.0274 | GRU-HLR: 0.0307;DHP: 0.0779;HLR: 0.1070 |
| THLR预测 | MAPE | 16.70% | GRU-HLR: 18.88%;DHP: 46.35%;HLR: 76.65% |
| 仿真环境(EFC) | ARP | 0.920 | RANDOM: 0.867;ANKI: 0.637 |
| 仿真环境(DHP) | ARP | 0.942 | MEMORIZE: 0.817;ANKI: 0.584;RANDOM: 0.475;HLR: 0.468 |
| 仿真环境(THLR) | ARP | 0.372 | HLR: 0.355;ANKI: 0.185;RANDOM: 0.178;MEMORIZE: 0.169 |

DRL-SRS的贡献在于:一是用“仿真—学习—部署”的闭环,将预测与优化统一到同一框架;二是通过深度模型捕捉个体轨迹的时序动态,实现更细粒度的个性化调度[^9]。其局限在于:对数据质量与计算资源的要求较高,跨任务泛化与长期稳定性仍需更多实证。

### 算法比较与评估指标

综合来看,SM-2以“评分—参数—间隔”的闭环实现简单可解释;FSRS以概率预测为核心,在大规模数据上验证了精度优势并完成工程落地;SM-17在理论完备性上前进了一步,但受限于数据与实现细节的公开程度;DRL-SRS代表了“预测+优化”一体化与个体化的方向。评估指标方面,RMSE适合衡量概率预测的误差,ARP适合衡量调度策略在仿真或真实分布下的平均表现[^5][^7][^9]。表5给出算法特征对比。

表5 算法特征与评估对比(概览)

| 算法 | 核心思路 | 参数/结构 | 数据需求 | 评估指标 | 工程成熟度 |
|---|---|---|---|---|---|
| SM-2 | 评分驱动EF与间隔 | EF、I、q | 低 | 成功率/稳定度 | 高(广泛使用) |
| SM-17 | 理论到应用的转换 | 理论参数(未公开) | 中(生态内) | 精度/稳定性 | 中(公开细节有限) |
| FSRS | 概率预测驱动调度 | 预测器+RMSE优化 | 高(大规模) | RMSE/ARP | 高(Anki原生支持) |
| DRL-SRS | 强化学习优化调度 | THLR+DQN+LSTM | 高(仿真+真实) | ARP/MAE/MAPE | 中(研究—落地过渡) |

---

## 基于用户表现的间隔动态调整策略

从SM-2到FSRS与DRL-SRS,间隔调整策略沿着“评分驱动—概率驱动—策略优化”的路径演进。工程上,稳健的调度器需要处理评分噪声、跨等级信息损失与目标回忆概率的显式控制。

首先,SM-2以0–5分评分直接更新EF与间隔,失败(低评分)触发重学与缩短间隔。这一“局部更新”策略对短期学习有效,但在长期与个体差异面前显得粗粒度[^13]。其次,FSRS将评分与历史间隔转化为“预测R”的特征输入,通过RMSE最小化优化调度,使“何时复习”服从“目标回忆概率”的约束[^5][^7]。再次,DRL-SRS在仿真环境中学习“状态—动作—奖励”的映射,以ARP最大化为目标,捕捉跨日的时序依赖与个体差异[^9]。

评分等级与信息损失是工程落地中的关键细节。FSRS在设计上最多支持4个评分等级,而SuperMemo传统上使用6等级;当从6等级映射到4等级时,信息损失不可避免,需要在特征工程与损失函数设计中加以补偿[^5]。此外,目标回忆概率(如R=0.9)可作为调度器的“服务级别协议”(SLA):当预测R低于阈值时触发复习,高于阈值则延后,以平衡“保持率—学习成本”。表6给出一个实践中的评分到调度动作的映射示例(以4等级FSRS为例)。

表6 评分等级到调度动作的映射示例(工程实践)

| 评分(4级) | 解释 | 调度动作(示例) |
|---|---|---|
| 再次(Again) | 回忆失败 | 立即重学,设置短间隔(如1天),并降低相关稳定性参数 |
| 困难(Hard) | 回忆困难但正确 | 适度缩短下一间隔,维持较高复习频率 |
| 良好(Good) | 正常回忆 | 按预测R与目标阈值决定是否延后间隔 |
| 简单(Easy) | 非常轻松 | 延后间隔,加快整体进度 |

注:实际动作需结合FSRS的预测器与优化目标确定,表中为示意性策略[^5][^7]。

---

## 短期记忆向长期记忆转化的神经机制与SRS对接

从神经科学视角看,学习与记忆涉及多层级、多时间尺度的可塑性过程。海马体与新皮层之间的协作,突触可塑性(长时程增强LTP与长时程抑制LTD)、行为时间尺度突触可塑性(BTSP),以及抑制性可塑性支持的“重放”,共同构成了从短期到长期记忆的转化机制[^16][^17][^18]。这些机制为SRS“何时复习、如何巩固”的调度原则提供了生物学约束。

首先,海马体在短期记忆与记忆巩固中扮演关键角色。最新研究显示,人类海马体不仅参与长期记忆形成,也贡献于短期记忆过程,这支持了“提取—巩固”循环中 hippocampus–neocortex 协同的重要性[^16]。其次,突触层面的LTP/LTD与更高阶的BTSP提供了“强度抬升—噪声抑制”的生物学基础,使得反复提取与间隔重复能够在不同时间尺度上稳定记忆痕迹[^17]。再次,抑制性可塑性支持的重放机制,有助于在离线或休息时段“回放”序列经验,从而促进巩固与迁移,这与“夜间睡眠不复习但记忆仍在加工”的现象相呼应[^18]。最后,从计算神经科学角度,强化学习与记忆巩固可能通过“模拟—选择”的机制协同工作:在不同策略或表征之间进行选择,最终收敛到更有利于长期保持的表征组合[^19]。

这些机制对SRS调度的启示包括:将“日内—跨日”的节奏纳入调度(利用重放与巩固窗口)、在“困难—失败”项目中增加短期重复频率(对应LTP的快速增益)、在“稳定—简单”项目中拉长间隔(允许更长的巩固与整合)。当前不足在于:从“分子/突触/回路机制”到“调度参数”的定量映射仍缺乏直接实证,未来需要更细粒度的跨层建模与验证[^17][^18][^19]。

---

## 个性化复习时间与回忆概率预测算法

个性化预测的目标,是根据学习者的历史轨迹与项目特征,预测其在未来时刻的回忆概率R(t),并据此生成个性化复习计划。方法谱系从广义线性/非线性回归,到深度时序模型,再到强化学习优化。

- HLR(Half-Life Regression):以“半衰期”为参数,估计项目的记忆衰减速度,是早期可训练模型之一。
- DHP(Distributed Half-Life with Parameters):在HLR基础上引入分布与参数化扩展,改善在不同材料与人群上的拟合。
- GRU-HLR:用门控循环单元(GRU)对序列依赖建模,提高对个体轨迹的捕捉能力。
- THLR(Transformer-based Half-Life Regression):以Transformer编码时序上下文,结合半衰期回归目标,在记忆预测的MAE/MAPE上显著优于前述方法,是DRL-SRS的记忆预测模块[^9]。
- DRL-SRS:在THLR预测的基础上,构建仿真环境与奖励函数,以DQN+LSTM学习最优间隔策略,实现“预测+优化”一体化[^9]。

表7总结了不同模型的输入特征与性能(节选)。

表7 个性化预测模型对比(节选)

| 模型 | 输入特征(示例) | 训练目标 | MAE | MAPE |
|---|---|---|---|---|
| HLR | 间隔、历史评分、半衰期 | 回归 | 0.1070 | 76.65% |
| DHP | HLR特征+分布参数 | 回归 | 0.0779 | 46.35% |
| GRU-HLR | 序列特征(GRU编码) | 回归 | 0.0307 | 18.88% |
| THLR | 序列上下文(Transformer) | 回归 | 0.0274 | 16.70% |

在语言学习的特定场景,研究还探索了“遗忘曲线模型与语言学特征”的结合,以预测词汇回忆概率,表明“材料特征+个体轨迹”的融合能进一步提升预测与调度效果[^20]。工程落地上,建议以RMSE/MAE评估预测误差,以ARP评估调度效果,并在“目标回忆概率(如R=0.9)”约束下优化间隔,兼顾保持率与学习成本[^5][^7][^9]。

---

## 大规模数据与性能比较:FSRS基准与实践

在Anki生态的2万集合与7.38亿次复习数据上,FSRS v4以默认参数在RMSE上优于SM-2,并在与SM-17等算法的对比中取得更优或相当的精度。需要注意的是,SM-17的完整实现未公开,且评分等级差异(4级vs 6级)会带来信息损失与比较偏差;即便如此,FSRS仍显示出稳健的优势[^4][^5][^7][^8]。另一方面,SuperMemo生态的LSTM预测模型展示了在自有数据上的可行性,提示“深度时序模型+大规模数据”的组合是提升预测与调度精度的重要路径[^21]。

表8汇总了FSRS与相关算法在公开描述中的表现要点(定性概览)。

表8 FSRS与SM-2/SM-17的性能比较(定性概览)

| 算法 | 数据来源与规模 | 评分等级 | 评估指标 | 相对表现 |
|---|---|---|---|---|
| FSRS v4 | Anki生态,2万集合,7.38亿复习 | ≤4级 | RMSE | 默认参数优于SM-2;整体优于/接近SM-17 |
| SM-2 | 通用 | 多样 | 成功率/稳定度 | 基线表现 |
| SM-17 | SuperMemo生态(公开细节有限) | 6级 | 精度/稳定性 | 精度高,但外部可复现性受限 |

实践建议包括:在统一数据集上建立标准化评测流程;明确评分等级映射策略与信息损失补偿;持续监控RMSE与ARP并用于参数更新与策略迭代[^5][^7][^8]。

---

## 实践指南:从理论到工程落地

从工程实现角度,一个可操作的SRS调度器应包含“数据采集—评分策略—参数初始化—预测与优化—评估与监控—隐私保护”的完整链路。

- 数据采集:仅收集调度所需最小字段,如卡片ID、评分与间隔长度,避免采集媒体与卡片内容,以降低隐私风险并满足合规要求[^5]。
- 评分策略:制定清晰的评分标准与映射(如4等级:Again/Hard/Good/Easy),并在系统内保持一致性;对跨等级迁移场景(如6→4)制定信息损失补偿策略[^5]。
- 参数初始化:对EF、半衰期或稳定性参数采用稳健默认值,结合个体轨迹在在线学习过程中逐步更新。
- 预测与优化:以RMSE/MAE优化预测器,以ARP或目标回忆概率(如R=0.9)优化调度;对“失败—重学—短间隔”的项目设置保护阈值,避免“雪崩效应”。
- 评估与监控:离线RMSE/ARP与在线A/B测试结合;建立“预测误差—调度动作—学习效果”的闭环监控。
- 隐私与合规:遵循数据最小化原则,明确数据保留策略与用户授权;在跨平台同步时进行去标识化处理[^5]。

表9给出一个工程实现检查清单(示例)。

表9 SRS工程实现检查清单(示例)

| 模块 | 关键项 | 说明 |
|---|---|---|
| 数据 | 最小化字段 | 仅ID、评分、间隔;不含媒体/内容 |
| 评分 | 等级与映射 | 4等级标准与跨等级补偿 |
| 参数 | 初始化与更新 | 稳健默认+在线自适应 |
| 预测 | 模型与指标 | THLR/FSRS;RMSE/MAE |
| 优化 | 目标与动作 | R=0.9目标;ARP最大化 |
| 监控 | 离线与在线 | RMSE/ARP+A/B测试 |
| 隐私 | 合规与保留 | 去标识化;最小保留期限 |

---

## 结论与未来方向

本报告从遗忘曲线的数学模型出发,解释了“分布—阈值”机制如何导致经验曲线与个体遗忘率背离,并以“抬升分布—拉长间隔”的逻辑,阐明了间隔重复使曲线“变平”的本质。随后,我们系统比较了SM-2、SM-17、FSRS与DRL-SRS的原理与工程实现,指出FSRS在大规模数据上的精度优势与生态落地,以及DRL-SRS在“预测+优化”一体化上的潜力。神经科学方面,LTP/LTD/BTSP与抑制性可塑性支持的重放机制,为SRS的“日内—跨日”节奏与“失败—重学”策略提供了生物学约束。工程层面,我们提出了以RMSE/ARP与目标回忆概率为核心的评估与优化框架,并给出了可操作的实现清单。

面向未来,建议重点推进以下方向:第一,跨语言与跨任务的统一评测基准与开放数据,提升算法可复现性与泛化性;第二,评分等级映射的标准化,减少跨生态迁移时的信息损失;第三,神经机制与调度策略的定量耦合,建立从分子/突触/回路到调度参数的映射;第四,强化学习与概率预测的融合优化,探索“个体化—稳态化—可解释化”的统一框架;第五,工程上强化隐私保护与合规,实现“有效学习”与“安全治理”的平衡[^3][^9][^16]。

---

## 参考文献

[^1]: 遗忘曲线 - 维基百科. https://zh.wikipedia.org/wiki/%E9%81%97%E5%BF%98%E6%9B%B2%E7%BA%BF  
[^2]: SuperMemo—渐进学习最前沿! - 知乎. https://www.zhihu.com/column/c_1308886122672484352  
[^3]: Why Empirical Forgetting Curves Deviate from Forgetting Rates: A Distribution Model of Memory. MDPI, 2025. https://www.mdpi.com/2076-328X/15/7/924  
[^4]: FSRS现已成为世界上最精确的间隔重复算法* - 知乎. https://zhuanlan.zhihu.com/p/671340049  
[^5]: ABC of FSRS - GitHub Wiki. https://github.com/open-spaced-repetition/fsrs4anki/wiki/ABC-of-FSRS  
[^6]: FSRS配置指南 - GitHub Docs. https://github.com/open-spaced-repetition/fsrs4anki/blob/main/docs/tutorial.md  
[^7]: FSRS基准测试仓库 - GitHub. https://github.com/open-spaced-repetition/fsrs-benchmark  
[^8]: 优化间隔重复调度的随机最短路径算法(墨墨背单词论文) - 墨墨百科. https://memodocs.maimemo.com/docs/2022_KDD  
[^9]: DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Scheduling. MDPI Applied Sciences, 2024. https://www.mdpi.com/2076-3417/14/13/5591  
[^10]: 09 1994:遗忘的指数性质 - 看云(Supermemo.guru 翻译计划). https://www.kancloud.cn/ankigaokao/supermemo-guru-cn/1895548  
[^11]: Evidence of flattening forgetting curves with spaced repetition and formulas to... Psychology & Neuroscience Stack Exchange. https://psychology.stackexchange.com/questions/30575/evidence-of-flattening-forgotten-curves-with-spaced-repetition-and-formulas-to  
[^12]: Adaptive Forgetting Curves for Spaced Repetition Language Learning. PMC. https://pmc.ncbi.nlm.nih.gov/articles/PMC7334729/  
[^13]: 间隔重复记忆法SM-2算法的实现与应用 - CSDN博客. https://blog.csdn.net/sunyata2022/article/details/131086320  
[^14]: Algorithm SM-17 - supermemo.guru. https://supermemo.guru/wiki/Algorithm_SM-17  
[^15]: SuperMemo: SuperMemo Algorithm (SM-17). https://super-memory.com/archive/help17//smalg.htm  
[^16]: The human hippocampus contributes to short-term memory. Brain, 2024. https://academic.oup.com/brain/article/147/8/2593/7709512  
[^17]: Diverse synaptic mechanisms underlying learning and memory consolidation. ScienceDirect, 2025. https://www.sciencedirect.com/science/article/pii/S0959438825000273  
[^18]: Inhibitory plasticity supports replay and consolidation. Nature Neuroscience, 2024. https://www.nature.com/articles/s41593-024-01745-w  
[^19]: Memory consolidation from a reinforcement learning perspective. Frontiers in Computational Neuroscience, 2024. https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2024.1538741/full  
[^20]: Adaptive Forgetting Curves for Spaced Repetition Language Learning(项目页). University of Warwick. https://warwick.ac.uk/fac/cross_fac/eduport/edufund/projects/yang/projects/adaptive-forgetting-curves-for-spaced-repetition-language-learning/  
[^21]: Modeling Spaced Repetition with LSTMs - SuperMemo(白皮书). https://www.supermemo.com/wp-content/uploads/SuperMemo_AI.pdf